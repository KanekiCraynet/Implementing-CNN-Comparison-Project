{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udde0 Perbandingan Arsitektur CNN dengan Dataset MNIST\n",
        "## LeNet-5 vs AlexNet vs GoogleNet vs ResNet\n",
        "\n",
        "Notebook ini membandingkan performa 4 arsitektur CNN klasik pada dataset MNIST.\n",
        "\n",
        "**References:**\n",
        "- LeNet-5: LeCun et al., 1998\n",
        "- AlexNet: Krizhevsky et al., 2012\n",
        "- GoogLeNet: Szegedy et al., 2014\n",
        "- ResNet: He et al., 2015\n",
        "\n",
        "> \u26a0\ufe0f **Pastikan Runtime menggunakan GPU**: Runtime > Change runtime type > GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Setup & Configuration\nInstall dependencies dan import libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install jika diperlukan (uncomment di Colab)\n# !pip install torch torchvision matplotlib seaborn pandas -q\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport time\nimport warnings\nfrom collections import OrderedDict\nwarnings.filterwarnings('ignore')\n\n# Configuration\nCONFIG = {\n    'batch_size': 64,\n    'epochs': 15,\n    'learning_rate': 0.001,\n    'image_size': 32,  # Unified size for fair comparison\n    'train_split': 0.8,\n    'patience': 5,  # Early stopping patience\n    'seed': 42\n}\n\n# Set seed for reproducibility\ntorch.manual_seed(CONFIG['seed'])\nnp.random.seed(CONFIG['seed'])\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Data Preparation\nLoad MNIST dan preprocessing dengan ukuran unified 32x32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transform = transforms.Compose([\n    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Download MNIST\nfull_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n# Split train/validation\ntrain_size = int(CONFIG['train_split'] * len(full_train))\nval_size = len(full_train) - train_size\ntrain_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(val_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Model Architectures\nImplementasi LeNet-5, AlexNet, GoogleNet, dan ResNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LeNet-5\nclass LeNet5(nn.Module):\n    \"\"\"LeNet-5 (LeCun et al., 1998) - Arsitektur CNN klasik\"\"\"\n    def __init__(self):\n        super(LeNet5, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n    \n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, 16 * 6 * 6)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n# AlexNet (Adapted for MNIST)\nclass AlexNet(nn.Module):\n    \"\"\"AlexNet (Krizhevsky et al., 2012) - Adapted untuk 1 channel\"\"\"\n    def __init__(self):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 64, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 192, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(192, 384, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(256 * 4 * 4, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, 10),\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x)\n\n# Inception Module for GoogleNet\nclass InceptionModule(nn.Module):\n    def __init__(self, in_ch, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool):\n        super(InceptionModule, self).__init__()\n        self.branch1 = nn.Sequential(\n            nn.Conv2d(in_ch, out_1x1, 1), nn.ReLU(inplace=True))\n        self.branch2 = nn.Sequential(\n            nn.Conv2d(in_ch, red_3x3, 1), nn.ReLU(inplace=True),\n            nn.Conv2d(red_3x3, out_3x3, 3, padding=1), nn.ReLU(inplace=True))\n        self.branch3 = nn.Sequential(\n            nn.Conv2d(in_ch, red_5x5, 1), nn.ReLU(inplace=True),\n            nn.Conv2d(red_5x5, out_5x5, 5, padding=2), nn.ReLU(inplace=True))\n        self.branch4 = nn.Sequential(\n            nn.MaxPool2d(3, stride=1, padding=1),\n            nn.Conv2d(in_ch, out_pool, 1), nn.ReLU(inplace=True))\n    \n    def forward(self, x):\n        return torch.cat([self.branch1(x), self.branch2(x), \n                         self.branch3(x), self.branch4(x)], 1)\n\n# GoogleNet (Simplified)\nclass GoogleNet(nn.Module):\n    \"\"\"GoogLeNet/Inception v1 (Szegedy et al., 2014) - Simplified\"\"\"\n    def __init__(self):\n        super(GoogleNet, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 64, 7, stride=2, padding=3),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(3, stride=2, padding=1))\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64, 192, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(3, stride=2, padding=1))\n        self.inception3a = InceptionModule(192, 64, 96, 128, 16, 32, 32)\n        self.inception3b = InceptionModule(256, 128, 128, 192, 32, 96, 64)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(480, 10)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.inception3a(x)\n        x = self.inception3b(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\n# ResNet Basic Block\nclass BasicBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_ch)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_ch)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_ch != out_ch:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_ch))\n    \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        return F.relu(out)\n\n# ResNet-18 (Simplified)\nclass ResNet(nn.Module):\n    \"\"\"ResNet-18 (He et al., 2015) - Simplified untuk MNIST\"\"\"\n    def __init__(self):\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 64, 3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True))\n        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(256, 10)\n    \n    def _make_layer(self, in_ch, out_ch, blocks, stride):\n        layers = [BasicBlock(in_ch, out_ch, stride)]\n        for _ in range(1, blocks):\n            layers.append(BasicBlock(out_ch, out_ch))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Training & Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def count_parameters(model):\n    \"\"\"Hitung jumlah parameters yang trainable\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    for images, labels in loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    return total_loss / len(loader), 100. * correct / total\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss, correct, total = 0, 0, 0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return total_loss / len(loader), 100. * correct / total, all_preds, all_labels\n\ndef train_model(model, name, train_loader, val_loader, device, config):\n    \"\"\"Training dengan early stopping dan LR scheduler\"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"Training {name}\")\n    print(f\"Parameters: {count_parameters(model):,}\")\n    print(f\"{'='*50}\")\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n    \n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n    best_val_acc = 0\n    patience_counter = 0\n    start_time = time.time()\n    \n    for epoch in range(config['epochs']):\n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n        scheduler.step(val_loss)\n        \n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        \n        print(f\"Epoch {epoch+1:2d}/{config['epochs']} | \"\n              f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | \"\n              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%\")\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            patience_counter = 0\n            best_model_state = model.state_dict().copy()\n        else:\n            patience_counter += 1\n            if patience_counter >= config['patience']:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n    \n    training_time = time.time() - start_time\n    model.load_state_dict(best_model_state)\n    \n    return model, history, training_time, best_val_acc"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Train All Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "models_dict = OrderedDict([\n    ('LeNet-5', LeNet5()),\n    ('AlexNet', AlexNet()),\n    ('GoogleNet', GoogleNet()),\n    ('ResNet', ResNet())\n])\n\nresults = {}\nhistories = {}\n\nfor name, model in models_dict.items():\n    model = model.to(device)\n    trained_model, history, train_time, best_acc = train_model(\n        model, name, train_loader, val_loader, device, CONFIG)\n    \n    # Test evaluation\n    criterion = nn.CrossEntropyLoss()\n    test_loss, test_acc, preds, labels = evaluate(trained_model, test_loader, criterion, device)\n    \n    # Inference time\n    trained_model.eval()\n    dummy = torch.randn(1, 1, CONFIG['image_size'], CONFIG['image_size']).to(device)\n    with torch.no_grad():\n        start = time.time()\n        for _ in range(100):\n            _ = trained_model(dummy)\n        inf_time = (time.time() - start) / 100 * 1000  # ms\n    \n    results[name] = {\n        'params': count_parameters(trained_model),\n        'train_time': train_time,\n        'test_acc': test_acc,\n        'inference_time': inf_time,\n        'predictions': preds,\n        'labels': labels\n    }\n    histories[name] = history\n    \n    # Clear memory\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Training Complete!\")\nprint(\"=\"*50)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Results Comparison Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "comparison_df = pd.DataFrame({\n    'Model': list(results.keys()),\n    'Parameters': [f\"{results[m]['params']:,}\" for m in results],\n    'Train Time (s)': [f\"{results[m]['train_time']:.1f}\" for m in results],\n    'Test Accuracy (%)': [f\"{results[m]['test_acc']:.2f}\" for m in results],\n    'Inference (ms)': [f\"{results[m]['inference_time']:.2f}\" for m in results]\n})\n\nprint(\"\\n\ud83d\udcca Model Comparison Results\\n\")\nprint(comparison_df.to_string(index=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\ncolors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c']\n\n# Training Loss\nax1 = axes[0, 0]\nfor i, (name, hist) in enumerate(histories.items()):\n    ax1.plot(hist['train_loss'], label=name, color=colors[i], linewidth=2)\nax1.set_title('Training Loss', fontsize=12, fontweight='bold')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Validation Accuracy\nax2 = axes[0, 1]\nfor i, (name, hist) in enumerate(histories.items()):\n    ax2.plot(hist['val_acc'], label=name, color=colors[i], linewidth=2)\nax2.set_title('Validation Accuracy', fontsize=12, fontweight='bold')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Test Accuracy Bar Chart\nax3 = axes[1, 0]\naccs = [results[m]['test_acc'] for m in results]\nbars = ax3.bar(results.keys(), accs, color=colors)\nax3.set_title('Test Accuracy Comparison', fontsize=12, fontweight='bold')\nax3.set_ylabel('Accuracy (%)')\nax3.set_ylim([min(accs)-1, 100])\nfor bar, acc in zip(bars, accs):\n    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n             f'{acc:.2f}%', ha='center', fontsize=10)\n\n# Parameters vs Accuracy\nax4 = axes[1, 1]\nparams = [results[m]['params'] for m in results]\nfor i, name in enumerate(results.keys()):\n    ax4.scatter(params[i], accs[i], s=200, c=colors[i], label=name, zorder=5)\nax4.set_title('Parameters vs Accuracy', fontsize=12, fontweight='bold')\nax4.set_xlabel('Number of Parameters')\nax4.set_ylabel('Test Accuracy (%)')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('results/comparison_plots.png', dpi=150, bbox_inches='tight')\nplt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Confusion Matrix untuk Setiap Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\naxes = axes.flatten()\n\nfor idx, (name, res) in enumerate(results.items()):\n    cm = confusion_matrix(res['labels'], res['predictions'])\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], \n                xticklabels=range(10), yticklabels=range(10))\n    axes[idx].set_title(f'{name} Confusion Matrix', fontsize=12, fontweight='bold')\n    axes[idx].set_xlabel('Predicted')\n    axes[idx].set_ylabel('Actual')\n\nplt.tight_layout()\nplt.savefig('results/confusion_matrices.png', dpi=150, bbox_inches='tight')\nplt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Per-Class Accuracy Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n\nprint(\"\\n\ud83d\udcca Per-Class Metrics (Best Model)\\n\")\nbest_model = max(results.keys(), key=lambda x: results[x]['test_acc'])\nprint(f\"Best Model: {best_model} ({results[best_model]['test_acc']:.2f}%)\\n\")\n\nprec, rec, f1, _ = precision_recall_fscore_support(\n    results[best_model]['labels'], \n    results[best_model]['predictions'], \n    average=None\n)\n\nmetrics_df = pd.DataFrame({\n    'Digit': range(10),\n    'Precision': [f\"{p:.4f}\" for p in prec],\n    'Recall': [f\"{r:.4f}\" for r in rec],\n    'F1-Score': [f\"{f:.4f}\" for f in f1]\n})\nprint(metrics_df.to_string(index=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Summary & Kesimpulan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*60)\nprint(\"\ud83d\udcdd KESIMPULAN\")\nprint(\"=\"*60)\n\nbest = max(results.keys(), key=lambda x: results[x]['test_acc'])\nfastest = min(results.keys(), key=lambda x: results[x]['inference_time'])\nsmallest = min(results.keys(), key=lambda x: results[x]['params'])\n\nprint(f\"\"\"\n\ud83c\udfc6 Best Accuracy: {best} ({results[best]['test_acc']:.2f}%)\n\u26a1 Fastest Inference: {fastest} ({results[fastest]['inference_time']:.2f}ms)\n\ud83d\udce6 Smallest Model: {smallest} ({results[smallest]['params']:,} params)\n\n\ud83d\udcca Trade-off Analysis:\n- LeNet-5: Paling sederhana, cocok untuk deployment dengan resource terbatas\n- AlexNet: Lebih dalam, performa bagus tapi parameter banyak\n- GoogleNet: Inception modules efisien, balance antara akurasi dan kompleksitas\n- ResNet: Skip connections membantu training, performa stabil\n\"\"\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}